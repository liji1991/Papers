# ASAP：对齐仿真与现实物理以学习敏捷人形机器人全身技能

> **论文信息**
> - 标题：ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills
> - 作者：Tairan He†, Jiawei Gao†, Wenli Xiao†, Yuanhang Zhang†, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He 等（CMU & NVIDIA，†为共同一作）
> - 发表：arXiv:2502.01143v3 [cs.RO]，2025年4月26日
> - 项目主页：https://agile.human2humanoid.com
> - 代码：https://github.com/LeCAR-Lab/ASAP

---

## 一、核心问题与动机

### 1.1 背景

人形机器人被认为具有执行人类全身技能的巨大潜力，但实现**敏捷、协调的全身运动**仍是重大挑战。现有研究大多聚焦于行走/移动，即把双腿作为纯移动工具，对需要跳跃、踢球、单腿平衡等高动态全身技能的探索还很有限。

### 1.2 核心障碍：Sim-to-Real 动力学鸿沟

在仿真（Simulation）中训练好的控制策略，直接部署到真实机器人（Real World）时性能往往会大幅下降，根本原因是**仿真与现实世界之间存在动力学不匹配（Dynamics Mismatch）**，包括：

- 电机响应特性建模不准确
- 传感器噪声被忽略
- 机器人的质量、惯量参数存在偏差
- 关节摩擦、齿隙等非线性特性难以精确建模
- 接触力学（地面摩擦、碰撞）建模误差

### 1.3 现有方法的局限

| 方法 | 原理 | 局限性 |
|------|------|--------|
| **系统辨识（SysID）** | 标定仿真中的物理参数，使之贴近真机 | 需要预定义参数空间，难以覆盖所有差距；通常需要扭矩测量，硬件不一定支持 |
| **域随机化（DR）** | 在仿真中随机化参数，提升策略鲁棒性 | 导致策略过于保守，牺牲敏捷性 |
| **学习动力学模型** | 学习真实世界的残差动力学模型 | 在无人机等低维系统有效，对高维人形机器人效果未经验证 |

### 1.4 ASAP 的核心思路

提出 **ASAP（Aligning Simulation and Real Physics）**，一个**两阶段框架**：

1. **预训练阶段**：在仿真中从人体视频数据学习敏捷全身运动追踪策略
2. **后训练阶段**：将策略部署到真机采集数据，训练 **Delta 动作模型**（残差动作模型），补偿仿真与现实的动力学差距，然后在对齐后的仿真中微调策略

最终策略可**直接部署到真机，无需携带 Delta 模型**，既保证了敏捷性，又显著降低了动作追踪误差（真机实验最高降低 **52.7%**）。

---

## 二、方法详解

ASAP 的完整流程如图所示（共 4 个阶段）：

```
人体视频数据集
     ↓ 姿态估计 + 动作重定向
机器人参考轨迹数据集
     ↓ 强化学习训练
预训练动作追踪策略
     ↓ 真机部署 + 数据采集
真实世界轨迹 Dr
     ↓ RL 训练 Delta 动作模型
Delta 动作模型 π^Δ
     ↓ 冻结 π^Δ，嵌入仿真器
对齐后的仿真器 f^ASAP
     ↓ 策略微调
最终策略（直接部署到真机）
```

### 2.1 预训练阶段：学习敏捷人形技能

#### 2.1.1 数据生成：人体视频重定向到机器人运动

**目标**：从真实人体动作中获取丰富的参考运动轨迹，作为运动追踪策略的模仿目标。

**三步流程**：

**Step 1：视频 → SMPL 人体运动**

- 拍摄人类完成各种敏捷动作（如 C 罗的跳跃庆祝、勒布朗的单腿平衡等）的视频
- 使用 **TRAM** 工具从视频重建全局三维运动，输出 **SMPL 格式**的运动参数（包含全局根部平移、朝向、身体姿态、形状参数）
- 得到 SMPL 运动数据集 $\mathcal{D}_{SMPL}$

**Step 2：仿真清洗（sim-to-data cleaning）**

- 视频重建引入的噪声可能使部分动作在物理上不可行
- 使用 **MaskedMimic**（一个基于物理的运动追踪器）在 Isaac Gym 中过滤掉物理不可行的动作片段
- 得到清洗后的数据集 $\mathcal{D}_{SMPL}^{Cleaned}$

**Step 3：SMPL 运动 → 机器人运动（形状-运动两阶段重定向）**

- 由于 SMPL 表示各种人体体型，首先通过梯度下降优化形状参数 $\beta'$，近似人形机器人的体型（对齐 12 个身体部位）
- 然后再次梯度下降，最小化对应身体部位的关节距离，将 SMPL 动作映射为机器人关节空间的参考轨迹
- 得到机器人参考轨迹数据集 $\mathcal{D}_{Robot}^{Cleaned}$

#### 2.1.2 基于相位的运动追踪策略训练

**任务定义**：将运动追踪问题构建为目标条件强化学习（Goal-Conditioned RL），训练策略 $\pi$ 追踪参考轨迹。

**状态空间**：策略的输入状态 $s_t$ 由两部分组成：

$$s_t = (s_t^p, \phi)$$

- $s_t^p$（本体感知）：包含 5 步历史的关节位置 $q_t \in \mathbb{R}^{23}$、关节速度 $\dot{q}_t$、根部角速度 $\omega_t^{root}$、根部重力投影 $g_t$、上一步动作 $a_{t-1}$
- $\phi \in [0, 1]$（**时间相位变量**）：$\phi=0$ 表示运动起始，$\phi=1$ 表示运动结束，**单独用相位就足以作为目标状态**

**动作空间**：$a_t \in \mathbb{R}^{23}$，输出 23 个关节的目标位置，传递给 PD 控制器驱动机器人

**优化算法**：PPO（Proximal Policy Optimization），最大化累计折扣奖励

**四大关键设计：**

---

**① 非对称 Actor-Critic 训练**

真实世界中，某些任务相关信息在仿真中可以获取，但在真机上无法观测（如参考运动的全局位置、根部线速度）。

解决方案：
- **Critic 网络**：可访问特权信息（参考运动全局位置、根部线速度）
- **Actor 网络**：仅使用本体感知输入 + 时间相位变量

这样设计的好处：
- 训练时 Critic 利用特权信息提升值函数估计质量，加速收敛
- Actor 不依赖位置类运动目标，**真机部署无需里程计**，克服了现有方法的一大痛点

---

**② 追踪容差终止课程（Termination Curriculum）**

训练高动态运动（如跳跃）时，策略常常早早失败，学会"站在原地"来规避落地惩罚。

解决方案：
- 训练初期：终止阈值设为 **1.5m**（机器人与参考运动偏差超过此值才终止）
- 随训练进行：逐渐收紧至 **0.3m**

这样课程让策略先学会基础平衡，再逐步提升追踪精度，最终学会高动态全身动作。

---

**③ 参考状态初始化（Reference State Initialization, RSI）**

朴素地从动作起始点开始训练（如 C 罗跳跃）会导致策略学习串行化——必须先学起跳才能学落地。但一个成功的空翻必须**先学会落地**。

解决方案：
- 随机采样 $\phi \in [0, 1]$，初始化机器人到对应相位的参考运动状态（根部位置/朝向、速度、关节状态）
- 策略可**并行学习运动的所有阶段**（起跳、腾空、落地、旋转等）

---

**④ 奖励函数设计**

奖励由三部分组成：

| 类别 | 具体项 | 权重 |
|------|--------|------|
| **惩罚项** | DoF 位置限制超限 | −10.0 |
| | 扭矩限制超限 | −5.0 |
| | DoF 速度限制超限 | −5.0 |
| | 终止惩罚 | −200.0 |
| | 扭矩 | −1×10⁻⁶ |
| | 足端朝向 | −2.0 |
| | 足端滑动 | −1.0 |
| **正则化项** | 动作变化率 | −0.5 |
| | 足端朝向 | −0.1 |
| **任务奖励** | 身体位置追踪 | 1.0 |
| | 足端位置追踪 | 2.1 |
| | 身体角速度追踪 | 0.5 |
| | 关节位置追踪 | 0.75 |
| | VR 三点追踪 | 1.6 |
| | 身体旋转追踪 | 0.5 |
| | 身体速度追踪 | 0.5 |
| | DoF 速度追踪 | 0.5 |

---

**⑤ 域随机化（Domain Randomization）**

在预训练阶段还使用基础域随机化提升鲁棒性：

| 参数 | 随机范围 |
|------|---------|
| 地面摩擦系数 | $U(0.2, 1.1)$ |
| P 增益 | $U(0.925, 1.05) \times$ 默认值 |
| 控制延迟 | $U(20, 40)$ ms |
| 外部推力扰动 | 每 10s 一次，$v_{xy} = 0.5$ m/s |

---

### 2.2 后训练阶段：训练 Delta 动作模型与策略微调

#### 2.2.1 真机数据采集

将预训练策略部署到真机，执行全身运动追踪任务，采集真实世界轨迹：

$$\mathcal{D}_r = \{s_0^r, a_0^r, \ldots, s_T^r, a_T^r\}$$

每个时刻 $t$ 记录的状态：

$$s_t = [p_t^{base}, v_t^{base}, \alpha_t^{base}, \omega_t^{base}, q_t, \dot{q}_t]$$

- $p_t^{base} \in \mathbb{R}^3$：机器人基座三维位置（由动捕系统记录）
- $v_t^{base} \in \mathbb{R}^3$：基座线速度
- $\alpha_t^{base} \in \mathbb{R}^4$：基座四元数朝向
- $\omega_t^{base} \in \mathbb{R}^3$：基座角速度
- $q_t \in \mathbb{R}^{23}$：关节位置向量
- $\dot{q}_t \in \mathbb{R}^{23}$：关节速度向量

#### 2.2.2 Delta 动作模型训练（核心创新）

**核心思路**：

由于仿真与现实的动力学差距，当我们在仿真中**重放真实世界的轨迹**时，仿真轨迹会与真实轨迹产生偏差。这一偏差正是学习动力学差距的宝贵信号。

**Delta 动作模型定义**：

$$\Delta a_t = \pi_\theta^\Delta(s_t, a_t)$$

- 输入：当前状态 $s_t$ 和原始动作 $a_t$
- 输出：修正动作量 $\Delta a_t$
- 修正后的仿真动态：$s_{t+1} = f^{sim}(s_t, a_t^r + \Delta a_t)$

**训练过程（RL 训练）**：

每个 RL 步骤：

1. 机器人初始化到真实世界状态 $s_t^r$
2. 奖励信号 = 最小化仿真状态 $s_{t+1}$ 与真实记录状态 $s_{t+1}^r$ 之间的偏差（加上动作幅度正则项 $\exp(-\|a_t\|) - 1$）
3. 用 **PPO** 训练 Delta 动作策略 $\pi_\theta^\Delta$，学习修正量 $\Delta a_t$

**直觉理解**：

设想仿真中的电机强度被高估，真机无法完成预期跳跃。Delta 动作模型会学习**减小下肢动作输出强度**，使仿真中的电机响应与真机一致，从而让仿真器能够重现真机的动力学行为（包括失败案例）。

**Delta 动作模型奖励函数**：

| 类别 | 项 | 权重 |
|------|-----|------|
| 惩罚 | DoF 位置限制 | −10.0 |
| | 扭矩限制 | −0.1 |
| | DoF 速度限制 | −5.0 |
| | 终止惩罚 | −200.0 |
| 正则化 | 动作变化率 | −0.01 |
| | **动作幅度（Action Norm）** | **−0.2** |
| 任务奖励 | 身体位置 | 1.0 |
| | 足端位置 | 1.0 |
| | 身体角速度 | 0.5 |
| | 关节位置 | 0.5 |
| | VR 三点 | 1.0 |
| | 身体旋转 | 0.5 |
| | 身体速度 | 0.5 |
| | DoF 速度 | 0.5 |

> 动作幅度惩罚 $-0.2 \cdot (\exp(-\|a_t\|) - 1)$ 是关键超参数，确保 Delta 修正量最小化，避免不必要的大幅修正。

#### 2.2.3 在对齐后的动力学下微调策略

构建对齐后的仿真环境：

$$s_{t+1} = f^{ASAP}(s_t, a_t) = f^{sim}(s_t, a_t + \pi^\Delta(s_t, a_t))$$

- **冻结** Delta 动作模型参数 $\pi^\Delta$
- 将 $\pi^\Delta$ 嵌入仿真器，在新的动力学下**微调预训练策略**（使用与预训练相同的奖励函数）
- 微调后的策略已内化了对真机动力学偏差的补偿

#### 2.2.4 真机部署

- **直接部署微调后的策略到真机**，**不携带 Delta 动作模型**
- 策略本身已包含对真机动力学的适配，无需额外计算开销
- 相比预训练策略，真机上的运动追踪质量显著提升

---

## 三、与 Baseline 对比

ASAP 与以下基线方法对比：

| Baseline | 描述 |
|----------|------|
| **Oracle** | 训练和测试均在 IsaacGym，代表性能上界 |
| **Vanilla** | 在 IsaacGym 训练，直接部署到目标环境 |
| **SysID** | 搜索最优仿真参数（质心偏移、质量偏移、PD 增益比）对齐目标环境 |
| **DeltaDynamics** | 学习残差**动力学模型** $f_\theta^\Delta(s_t, a_t)$ 预测状态差值 |
| **ASAP（本文）** | 学习残差**动作模型** $\pi_\theta^\Delta(s_t, a_t)$ 输出修正动作 |

**Delta Dynamics vs. ASAP 的关键区别**：

- DeltaDynamics：学习 $\Delta s_{t+1} = f_\theta^\Delta(s_t^r, a_t^r)$，即预测状态差值，在仿真时叠加状态偏移
- ASAP：学习 $\Delta a_t = \pi_\theta^\Delta(s_t, a_t)$，即预测动作修正量，在仿真时施加修正动作

DeltaDynamics 存在过拟合和长序列误差累积问题；ASAP 的 RL 方式学习残差动作更鲁棒。

---

## 四、实验结果

### 4.1 评估指标

- **Eg-mpjpe**（mm）：全局身体位置追踪误差（Mean Per Joint Position Error，全局）
- **Empjpe**（mm）：相对身体位置追踪误差（MPJPE，根部相对）
- **Eacc**（mm/frame²）：加速度误差
- **Evel**（mm/frame）：根部速度误差
- **成功率**：任意时刻平均身体距离误差不超过 0.5m 的比例

实验分三类任务难度：**Easy / Medium / Hard**（共 43 个动作序列）

### 4.2 仿真-仿真迁移（IsaacGym → IsaacSim / Genesis）

**开环回放性能**（同一轨迹在目标仿真器中的状态差异）：

| 轨迹长度 | 方法 | Eg-mpjpe (IsaacSim) | Eg-mpjpe (Genesis) |
|----------|------|---------------------|---------------------|
| 0.5s | OpenLoop | 33.3 | 33.1 |
| | SysID | 32.1 | 32.2 |
| | DeltaDynamics | 36.5 | 27.8 |
| | **ASAP** | **26.8** | **25.9** |
| 1.0s | OpenLoop | 80.8 | 82.5 |
| | SysID | 77.6 | 76.5 |
| | DeltaDynamics | 68.1 | 50.2 |
| | **ASAP** | **37.9** | **36.9** |

> **关键发现**：ASAP 在长序列（1.0s）上的优势最为明显，误差仅为 OpenLoop 的约 46%；而 SysID 在长时序下因累积误差表现不佳，DeltaDynamics 因过拟合出现级联误差放大。

**闭环策略微调性能**（在目标仿真器中评估微调后策略）：

| 难度 | 方法 | IsaacSim Eg-mpjpe | Genesis Eg-mpjpe |
|------|------|-------------------|-----------------|
| Easy | Vanilla | 107 | 140 |
| | SysID | 105 | 127 |
| | DeltaDynamics | 127 | 168 |
| | **ASAP** | **106** | **125** |
| Medium | Vanilla | 114 | 169 |
| | SysID | 115 | 138 |
| | DeltaDynamics | 151 | 190 |
| | **ASAP** | **112** | **126** |
| Hard | Vanilla | 148 | 175 |
| | SysID | 165 | 186 |
| | DeltaDynamics | 137 | 190 |
| | **ASAP** | **129** | **129** |

> ASAP 在所有难度等级和两个仿真器上均保持 **100% 成功率**，而 DeltaDynamics 在难度较高时成功率下降（最低 60%）。

### 4.3 真机实验（IsaacGym → Unitree G1）

**真机实验设置**：
- 5 个全身运动追踪任务：踢腿、向前跳跃、前后迈步、单腿平衡、单腿跳跃
- 由于高动态运动导致关节电机过热，**2 台 Unitree G1 在实验中损坏**
- 采用 **4-DoF 踝关节 Delta 动作模型**（而非完整 23-DoF），原因：
  1. 真机数据量有限（收集了 100 个动作片段），全 23-DoF 模型需要 400+ 片段
  2. G1 的踝关节采用机械连杆设计，仿真-真机差距最显著

**真机结果**（以踢腿和 LeBron "Silencer" 运动为例）：

| 运动 | 方法 | Eg-mpjpe | Empjpe | Eacc | Evel |
|------|------|----------|--------|------|------|
| Real-World-Kick | Vanilla | 61.2 | 43.5 | 2.96 | 2.91 |
| | **ASAP** | **50.2** | **40.1** | **2.46** | **2.70** |
| Real-World-LeBron (OOD) | Vanilla | 159 | 55.3 | 3.43 | 6.43 |
| | **ASAP** | **112** | **47.5** | **2.84** | **5.94** |

> **值得注意**：LeBron "Silencer" 动作是**分布外（OOD）**任务（未用于训练 Delta 模型），ASAP 仍能有效泛化，将全局追踪误差从 159mm 降至 112mm（降低 **29.6%**），证明了 Delta 动作模型的泛化能力。

**策略过渡（Policy Transition）**：

真机不像仿真器可以随意重置，ASAP 还训练了一个**鲁棒行走策略**用于不同运动追踪任务之间的过渡。行走策略输入指令 $(v, \omega, \Pi)$（线速度、角速度、行走/站立），在每次运动追踪结束后接管机器人，保持平衡直到下一任务开始。

---

## 五、深入分析

### 5.1 Delta 动作模型的关键训练因素（Q4）

**① 数据集大小**

- 增大数据集可提升 $\pi^\Delta$ 的分布外泛化能力（OOD 误差降低）
- 闭环性能在 4300 样本后趋于饱和，增至 43000 样本仅带来 **0.65%** 的额外提升
- 结论：**适度数据量即可，不必追求海量数据**

**② 训练 Horizon**

- 开环性能随 Horizon 延长持续改善，1.5s 时最低
- 闭环性能的最优 Horizon 为 **1.0s**，过长反而无益
- 结论：闭环部署使用 **1.0s 训练 Horizon**

**③ 动作幅度权重（Action Norm Weight）**

- 权重从 0 增大到 0.1 时，开环和闭环误差均降低
- 超过 0.1 后，开环误差上升（动作幅度惩罚主导了训练）
- 结论：最优权重约为 **0.1**，需要仔细调整

### 5.2 Delta 动作模型的使用方式（Q5）

给定学习到的 Delta 策略 $\pi^\Delta$，有三种使用方式将预训练策略 $\hat\pi(s)$ 适配到真机：

**① 不动点迭代（Fixed-Point Iteration）**

$$\pi(s) = \hat\pi(s) - \pi^\Delta(s, \pi(s))$$

初始化 $y_0 = \hat\pi(s)$，迭代更新 $y_{k+1} = \hat\pi(s) - \pi^\Delta(s, y_k)$

**② 梯度优化（Gradient-Based Optimization）**

最小化损失函数 $l(y) = \|y + \pi^\Delta(s, y) - \hat\pi(s)\|^2$

**③ RL 微调（本文方法）**

在对齐后的仿真器中用 RL 直接优化策略

**对比结果**：RL 微调取得最低追踪误差（MPJPE=126），方法①②因分布外问题（OOD issue）表现差于无微调的 baseline。

**原因分析**：方法①②基于**单步匹配假设**，无法处理多步依赖，且在有限数据上存在 OOD 问题。RL 微调则通过多步优化隐式地执行多步匹配，效果更优。

### 5.3 ASAP 与随机动作噪声微调的对比（Q6）

随机扭矩噪声（Random Torque Noise）是域随机化的常用手段，但它是否能替代 ASAP？

实验：在 IsaacGym 中用随机噪声 $\delta_a \sim U[0,1]$ 修改动力学 $s_{t+1} = f^{sim}(s_t, a_t + \beta\delta_a)$，调整噪声幅度 $\beta \in [0.025, 0.4]$

结果：
- 适当噪声（$\beta \in [0.025, 0.2]$）可改善性能（MPJPE 约 150）
- ASAP 的 MPJPE 为 **126**，显著优于最优随机噪声（**173**）

**关键发现（图 13）**：
可视化 $\pi^\Delta$ 的平均输出幅度，发现关节间差距**高度非均匀**：
- **下肢**电机比上肢有更大的动力学差距
- 踝关节（尤其踝关节俯仰）差距最大
- 左右身体还存在不对称性

> 这种结构化的非均匀差距无法被均匀随机噪声有效捕获，这正是 ASAP 胜过随机噪声的根本原因。

---

## 六、技术架构总结

### 6.1 ASAP 框架的本质

ASAP 是一种**残差动作学习（Residual Action Learning）** 框架，与传统残差动力学学习的核心区别：

```
Delta Dynamics（残差状态）：
    s_{t+1} = f^sim(s_t, a_t) + Δs_{t+1}    ← 修正状态

Delta Action（残差动作）[ASAP]：
    s_{t+1} = f^sim(s_t, a_t + Δa_t)         ← 修正动作
```

残差动作方法的优势：
1. 在标准仿真器内部修正，不改变仿真器本身
2. RL 训练天然避免长序列误差累积
3. 对分布外数据有更好的泛化能力

### 6.2 完整技术流程图

```
阶段一：预训练
┌─────────────────────────────────────────────────────┐
│  人体视频 → TRAM → SMPL运动                          │
│       ↓ MaskedMimic清洗                              │
│  清洗后SMPL → 两阶段重定向 → 机器人参考轨迹           │
│       ↓ PPO训练（Asymmetric AC + Curriculum + RSI）   │
│  预训练动作追踪策略 π̂                                 │
└─────────────────────────────────────────────────────┘
                         ↓
阶段二：真机部署 + 数据采集
┌─────────────────────────────────────────────────────┐
│  π̂ → 真机部署 → 动捕 + 本体传感 → 真实轨迹 Dr       │
└─────────────────────────────────────────────────────┘
                         ↓
阶段三：Delta 动作模型训练
┌─────────────────────────────────────────────────────┐
│  在仿真中重放真实轨迹 Dr                              │
│  RL训练 π^Δ 最小化 |s_sim - s_real|                  │
│  奖励：追踪奖励 + 动作幅度正则（Action Norm）         │
└─────────────────────────────────────────────────────┘
                         ↓
阶段四：策略微调 + 真机部署
┌─────────────────────────────────────────────────────┐
│  冻结 π^Δ，嵌入仿真器构建 f^ASAP                     │
│  在 f^ASAP 中微调 π̂ → 最终策略 π*                   │
│  π* 直接部署到真机（不携带 π^Δ）                     │
└─────────────────────────────────────────────────────┘
```

---

## 七、局限性与未来方向

### 7.1 当前局限

| 问题 | 描述 |
|------|------|
| **硬件风险** | 高动态运动导致关节电机严重过热，实验中 **2 台 G1 损坏**，限制了可安全采集的数据量和多样性 |
| **依赖动捕系统** | 真机数据采集需要 MoCap 系统记录轨迹，在无结构化环境中部署存在实际障碍 |
| **Delta 模型数据需求大** | 训练完整 23-DoF Delta 模型需要 400+ 个真机运动片段；目前退而求其次，仅训练 4-DoF 踝关节模型 |

### 7.2 未来方向

1. **损伤感知策略**：训练具有电机温度保护意识的策略，减少硬件损坏风险
2. **无动捕对齐（MoCap-free）**：探索仅使用机载传感器完成轨迹记录和对齐
3. **少样本 Delta 模型适应**：研究更高效的自适应技术，用少量真机数据完成 Delta 模型训练

---

## 八、核心贡献总结

| 贡献 | 内容 |
|------|------|
| **方法创新** | 提出 ASAP 两阶段框架，通过 RL 训练 Delta 动作模型弥合 Sim-to-Real 差距 |
| **技术突破** | 首次在真机上实现 RL 驱动的高难度全身控制（跳跃、踢腿、单腿平衡等），之前难以完成 |
| **实验验证** | 在 Sim-to-Sim（IsaacGym→IsaacSim/Genesis）和 Sim-to-Real（IsaacGym→G1）场景全面超越 Vanilla、SysID、DeltaDynamics |
| **工程贡献** | 开源多仿真器训练与评估代码库，支持 IsaacGym、IsaacSim、Genesis，促进研究社区发展 |

---

## 九、关键概念词汇表

| 英文术语 | 中文 | 含义 |
|----------|------|------|
| Sim-to-Real Gap | 仿真-现实差距 | 仿真与真实世界之间的动力学不匹配 |
| Delta Action Model | Delta 动作模型 | 学习动作修正量的残差策略网络 |
| Motion Retargeting | 运动重定向 | 将人体动作映射到机器人关节空间 |
| SMPL | 蒙皮多人线性模型 | 常用的参数化人体形状与姿态表示 |
| Asymmetric Actor-Critic | 非对称演员-评论家 | Actor 只用真机可观测信息，Critic 用特权信息 |
| RSI | 参考状态初始化 | 随机初始化到参考运动的任意相位 |
| Termination Curriculum | 终止课程 | 逐渐收紧运动偏差终止阈值的课程学习策略 |
| PPO | 近端策略优化 | 强化学习中的策略梯度算法 |
| MPJPE | 均值每关节位置误差 | 追踪误差的主要评估指标（mm） |
| SysID | 系统辨识 | 通过数据标定物理参数的方法 |
| Domain Randomization | 域随机化 | 随机化仿真参数增强鲁棒性的方法 |
| MoCap | 动作捕捉 | 用于记录真机轨迹的外部追踪系统 |
| DoF | 自由度 | 机器人关节可控制的维度数 |
| TRAM | TRAM | 从野外视频重建三维人体运动轨迹的工具 |

---

## 十、参考文献（精选）

- **ASAP 论文**：Tairan He et al., "ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills," arXiv:2502.01143, 2025
- **DeepMimic**（RSI, Phase-conditioned Policy）：Xue Bin Peng et al., "DeepMimic: Example-guided Deep Reinforcement Learning of Physics-based Character Skills," TOG 2018
- **TRAM**（视频人体动作重建）：Yufu Wang et al., "TRAM: Global Trajectory and Motion of 3D Humans from In-the-Wild Videos," ECCV 2025
- **MaskedMimic**（仿真运动清洗）：Chen Tessler et al., "MaskedMimic: Unified Physics-based Character Control through Masked Motion Inpainting," TOG 2024
- **Human2Humanoid**（重定向方法来源）：Tairan He et al., "Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation," arXiv:2403.04436, 2024
- **Isaac Gym**：Viktor Makoviychuk et al., "Isaac Gym: High Performance GPU Based Physics Simulation for Robot Learning," NeurIPS 2021
- **PPO**：John Schulman et al., "Proximal Policy Optimization Algorithms," arXiv:1707.06347, 2017
